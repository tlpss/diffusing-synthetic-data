
<h1 align="center">Evaluating Text-to-Image Diffusion Models for Texturing Synthetic Data </h1>


<p align="center">
  <a href="https://tlpss.github.io/">Thomas Lips</a>,
  <a href="https://airo.ugent.be/members/francis/">Francis wyffels</a>
  <br/>
  <a href="https://airo.ugent.be/">AI and Robotics Lab (AIRO) </a>,
  <a href="https://www.ugent.be/">Ghent University</a>
  <br/>
  <a href="https://arxiv.org/TOOD">paper </a>
<div align="center">
  <img src="docs/img/overview-figure-gh.jpg" width="70%">
</div>
</p>



Codebase for the paper *Evaluating Text-to-Image Diffusion Models
for Texturing Synthetic Data*.

In this paper we generate synthetic data to learn representations for robotic manipulation, using the following three steps:

1. Gather meshes of the target object categories and annotate them.
2. Generate 3D scenes for the objects, for which we can then generate the annotations.
3. Use text-to-image diffusion models to texture images of the scenes, or use random textures.


The codebase covers all steps and can hence be used to generate synthetic data for your objects.
It also contains all code to run the experiments from the paper and can be used to reproduce them.

# Codebase Overview

# Installation
<details>
<summary>expand here</summary>
</details>

# Generating Synthetic Data
<details>
<summary>expand here</summary>
</details>

# Reproducing Paper
<details>
<summary>expand here</summary>

## Trained Models

## Datasets

## Reproducing Experiments
</details>

# Development
<details>
<summary>expand here</summary>

blabla

</details>
